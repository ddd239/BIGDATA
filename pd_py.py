# -*- coding: utf-8 -*-
"""pd.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tGEOijAZn8O9A0n-eVatCIaQ6bUQ64qb
"""

import pandas as pd

from sklearn.preprocessing import MinMaxScaler
import numpy as np

train_array = np.arange(0,11).reshape(-1,1)
test_array = np.arange(0,6).reshape(-1,1)

scaler = MinMaxScaler()
scaler.fit(train_array)
train_scaled = scaler.transform(train_array)

housing_df['Age_0'] = 0
print(housing_df.head(3))

s = pd.Series([1,3,5,6,8])
print(s)
print(s.index)
print(s.values)

v = [[1,2,3],[4,5,6],[7,8,9]]
i = ['First','Second','Third']
c = ['Columm1','Columm2','Columm3']

df = pd.DataFrame(v, index=i, columns=c)
print(df)

housing_df = pd.read_csv('/content/sample_data/california_housing_train.csv')
print('housing variable type : ',type(housing_df))
print(housing_df)

print('단일 컬럼 데이터 조회 : \n',housing_df['housing_median_age'].head(3))
print('\n복수 컬럼 데이터 조회 : \n',housing_df[['housing_median_age','total_rooms']].head(3))

print(housing_df[housing_df['housing_median_age'] == 30].head(3))

housing_df['Age_by_10'] = housing_df['Age_by_10'] * 10
print(housing_df.head(3))

value_counts = housing_df['Age_by_10'].value_counts()
print(value_counts)

housing_drop_df = housing_df.drop('Age_0',axis=1)
print(housing_drop_df.head(3))

housing_df['Age_by_10'] = housing_df['housing_median_age'] //10
print(housing_df.head(3))

print(housing_df[
    (housing_df['housing_median_age'] > 30) &
    (housing_df['total_rooms'] < 100) &
    (housing_df['median_income'] > 10)
].head(3))

con1 = housing_df['housing_median_age'] > 30
con2 = housing_df['total_rooms'] < 100
con3 = housing_df['median_income'] > 10
print(housing_df[con1 & con2 & con3].head(3))

print(housing_df.iloc[0, 2])

print(housing_df.loc[0:4, 'housing_median_age'])

housing_sorted = housing_df.sort_values(by=['housing_median_age', 'total_rooms'], ascending=False)
print(housing_sorted.head(3))

print(housing_df.sum() / housing_df.count())

print(housing_df[['housing_median_age','total_rooms']].mean())

housing_groupby = housing_df.groupby('housing_median_age').mean()
print(housing_groupby.head(3))

housing_groupby = housing_df.groupby('housing_median_age')['total_rooms'].agg([min, max, sum])
print(housing_groupby.head(3))

import numpy as np

housing_df['Age_na'] = np.nan
print(housing_df.isna().head(3))

print(housing_df.isna().sum())

housing_df['Age_na'] = housing_df['Age_na'].fillna(housing_df['housing_median_age'].mean())
print(housing_df.head(3))

from sklearn.datasets import load_iris

iris = load_iris()
print('붓꽃 데이터세트 타입 : ',type(iris))
keys = iris.keys()
print('붓꽃 데이터세트 키', keys)

print('feature_names : ')
print(iris.feature_names)
print('\ntarget_names')
print(iris.target_names)
print('\ndata : ')
print(iris.data)
print('\ntarget : ')
print(iris.target)

from sklearn.preprocessing import LabelEncoder
items = ['TV','냉장고','전자레인지','컴퓨터','TV','냉장고','컴퓨터','컴퓨터']
encoder = LabelEncoder()
encoder.fit(items)
labels = encoder.transform(items)
print('change : ',labels)
print('class : ',encoder.classes_)

ori = encoder.inverse_transform([2,3,0,2,1,1])
print('origin : ', ori)

from sklearn.preprocessing import OneHotEncoder
import numpy as np

labels = labels.reshape(-1,1)
oh_encoder = OneHotEncoder()
oh_encoder.fit(labels)
oh_labels = oh_encoder.transform(labels)
print(oh_labels.toarray())
print(oh_labels.shape)

item_df = pd.DataFrame({'item' : items})
pd.get_dummies(item_df)

from sklearn.datasets import load_iris
import pandas as pd

iris = load_iris()
iris_data = iris.data
iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)
print(iris_df.mean())
print(iris_df.var())

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(iris_df)
iriscl = scaler.transform(iris_df)
iris_df_sc = pd.DataFrame(data = iriscl, columns = iris.feature_names)
print(iris_df_sc.mean())
print(iris_df_sc.var())

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(iris_df)
iris_scaled = scaler.transform(iris_df)
iris_df_scaled = pd.DataFrame(data = iris_scaled, columns = iris.feature_names)
print(iris_df_scaled.min())
print(iris_df_scaled.max())

from sklearn.preprocessing import MinMaxScaler
import numpy as np

train_array = np.arange(0,11).reshape(-1,1)
test_array = np.arange(0,6).reshape(-1,1)

scaler = MinMaxScaler()
scaler.fit(train_array)
train_scaled = scaler.transform(train_array)

print(np.round(train_array.reshape(-1),2))
print(np.round(train_scaled.reshape(-1),2))

test_scaled = scaler.transform(test_array)

print(np.round(test_array.reshape(-1),2))
print(np.round(test_scaled.reshape(-1),2))

!git clone https://github.com/AnalyticsKnight/yemoonsaBigdata

import pandas as pd

data = pd.read_csv("./yemoonsaBigdata/datasets/Part2/housing_data.csv")
col_names = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV','isHIighValue']
data.columns = col_names

print(data.head())

!git init

!git add .

!git commit -m "FIR"

!git config --global --list

!git init
!git add README.md
!git commit -m "first commit"
!git branch -M main
!git remote add origin https://github.com/ddd239/BIGDATA.git
!git push -u origin main